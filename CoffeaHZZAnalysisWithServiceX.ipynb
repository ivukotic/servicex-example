{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0815db08",
   "metadata": {},
   "source": [
    "This notebook is based on previous work carried out in the following repositories: \n",
    "- https://github.com/atlas-outreach-data-tools/notebooks-collection-opendata\n",
    "- https://github.com/gordonwatts/pyhep-2021-SX-OpenDataDemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b3fd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio, os\n",
    "import numpy as np\n",
    "from coffea.processor.servicex import Analysis, DataSource, LocalExecutor, DaskExecutor\n",
    "from func_adl import ObjectStream\n",
    "from func_adl_servicex import ServiceXSourceUpROOT\n",
    "from servicex.servicex import ServiceXDataset\n",
    "import hist\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import infofile # Same infofile used in https://github.com/atlas-outreach-data-tools/notebooks-collection-opendata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fe0fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# clean up files that may be left over from previous running (not needed, just to simplify debugging)\n",
    "for path in [\"histograms/\", \"figures/\"]:\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24197a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'root://eospublic.cern.ch//eos/opendata/atlas/OutreachDatasets/2020-01-22/4lep/'\n",
    "\n",
    "fileset = {'Data': [\n",
    "                    prefix + 'Data/data_A.4lep.root',\n",
    "                    prefix + 'Data/data_B.4lep.root',\n",
    "                    prefix + 'Data/data_C.4lep.root',\n",
    "                    prefix + 'Data/data_D.4lep.root'\n",
    "                    ],\n",
    "           'Background $Z,tt^{bar}$': [\n",
    "                                       prefix + 'MC/mc_361106.Zee.4lep.root',\n",
    "                                       prefix + 'MC/mc_361107.Zmumu.4lep.root',\n",
    "                                       prefix + 'MC/mc_410000.ttbar_lep.4lep.root'\n",
    "                                       ],\n",
    "           'Background $ZZ^{star}$': [prefix + 'MC/mc_363490.llll.4lep.root'],\n",
    "           'Signal ($m_H$ = 125 GeV)': [\n",
    "                                        prefix + 'MC/mc_345060.ggH125_ZZ4lep.4lep.root',\n",
    "                                        prefix + 'MC/mc_344235.VBFH125_ZZ4lep.4lep.root',\n",
    "                                        prefix + 'MC/mc_341964.WH125_ZZ4lep.4lep.root',\n",
    "                                        prefix + 'MC/mc_341947.ZH125_ZZ4lep.4lep.root'\n",
    "                                       ]}\n",
    "\n",
    "xsec_map = infofile.infos  # dictionary with event weighting information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2a6ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lumi = 10_000 # pb^-1 # data_A, data_B, data_C, data_D\n",
    "\n",
    "def get_xsec_weight(sample):\n",
    "    info = xsec_map[sample]\n",
    "    xsec_weight = (lumi*info[\"xsec\"])/(info[\"sumw\"]*info[\"red_eff\"]) #*1000 to go from fb-1 to pb-1\n",
    "    return xsec_weight  # return cross-section weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cd3ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mllll(lep_pt,lep_eta,lep_phi,lep_E):\n",
    "    # TODO: use vector here\n",
    "    # first lepton is [0], 2nd lepton is [1] etc\n",
    "    px_0 = lep_pt[:,0]*np.cos(lep_phi[:,0]) # x-component of lep[0] momentum\n",
    "    py_0 = lep_pt[:,0]*np.sin(lep_phi[:,0]) # y-component of lep[0] momentum\n",
    "    pz_0 = lep_pt[:,0]*np.sinh(lep_eta[:,0]) # z-component of lep[0] momentum\n",
    "    px_1 = lep_pt[:,1]*np.cos(lep_phi[:,1]) # x-component of lep[1] momentum\n",
    "    py_1 = lep_pt[:,1]*np.sin(lep_phi[:,1]) # y-component of lep[1] momentum\n",
    "    pz_1 = lep_pt[:,1]*np.sinh(lep_eta[:,1]) # z-component of lep[1] momentum\n",
    "    px_2 = lep_pt[:,2]*np.cos(lep_phi[:,2]) # x-component of lep[2] momentum\n",
    "    py_2 = lep_pt[:,2]*np.sin(lep_phi[:,2]) # y-component of lep[2] momentum\n",
    "    pz_2 = lep_pt[:,2]*np.sinh(lep_eta[:,2]) # z-component of lep[3] momentum\n",
    "    px_3 = lep_pt[:,3]*np.cos(lep_phi[:,3]) # x-component of lep[3] momentum\n",
    "    py_3 = lep_pt[:,3]*np.sin(lep_phi[:,3]) # y-component of lep[3] momentum\n",
    "    pz_3 = lep_pt[:,3]*np.sinh(lep_eta[:,3]) # z-component of lep[3] momentum\n",
    "    sumpx = px_0 + px_1 + px_2 + px_3 # x-component of 4-lepton momentum\n",
    "    sumpy = py_0 + py_1 + py_2 + py_3 # y-component of 4-lepton momentum\n",
    "    sumpz = pz_0 + pz_1 + pz_2 + pz_3 # z-component of 4-lepton momentum\n",
    "    sumE = lep_E[:,0] + lep_E[:,1] + lep_E[:,2] + lep_E[:,3] # energy of 4-lepton system\n",
    "    return np.sqrt(sumE**2 - sumpx**2 - sumpy**2 - sumpz**2)/1000 #/1000 to go from MeV to GeV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1515ec6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cut on lepton charge\n",
    "# paper: \"selecting two pairs of isolated leptons, each of which is comprised of two leptons with the same flavour and opposite charge\"\n",
    "def cut_lep_charge(lep_charge):\n",
    "# throw away when sum of lepton charges is not equal to 0\n",
    "# first lepton is [0], 2nd lepton is [1] etc\n",
    "    return ((lep_charge[:,0] + lep_charge[:,1] + lep_charge[:,2] + lep_charge[:,3]) != 0)\n",
    "\n",
    "# cut on lepton type\n",
    "# paper: \"selecting two pairs of isolated leptons, each of which is comprised of two leptons with the same flavour and opposite charge\"\n",
    "def cut_lep_type(lep_type):\n",
    "# for an electron lep_type is 11\n",
    "# for a muon lep_type is 13\n",
    "# throw away when none of eeee, mumumumu, eemumu\n",
    "    sum_lep_type = lep_type[:,0] + lep_type[:,1] + lep_type[:,2] + lep_type[:,3]\n",
    "    return ((sum_lep_type != 44) & (sum_lep_type != 48) & (sum_lep_type != 52))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f451f832",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HZZAnalysis(Analysis):\n",
    "    def process(self, events):\n",
    "        # Get dataset name and lepton information from events\n",
    "        dataset = events.metadata['dataset']\n",
    "        leptons = events.lep\n",
    "        \n",
    "        # Cut events based on lepton charge, then update leptons\n",
    "        events = events[~cut_lep_charge(leptons.charge)]\n",
    "        leptons = events.lep\n",
    "        \n",
    "        # Cut events based on lepton type, then update leptons\n",
    "        events = events[~cut_lep_type(leptons.typeid)]\n",
    "        leptons = events.lep\n",
    "        \n",
    "        # Calculate the mllll for each event\n",
    "        mllll = calc_mllll(leptons.pt, leptons.eta, leptons.phi, leptons.energy)\n",
    "        \n",
    "        if \"data_A\" in dataset:\n",
    "            # Create and fill a histogram for mllll\n",
    "            mllllhist = (\n",
    "                hist.Hist.new\n",
    "                .Reg(34, 80, 250, name='mllll', label='4-lepton invariant mass $\\mathrm{m_{4l}}$ [GeV]')\n",
    "                .StrCat(['Data'], name='dataset', label='Dataset')\n",
    "                .Weight()\n",
    "                .fill(mllll=mllll, dataset=\"Data\")\n",
    "            )\n",
    "        else:\n",
    "            # Extract the sample name from the filename metadata with regex\n",
    "            sample = re.findall(r'mc_\\d+\\.(.+)\\.4lep', events.metadata['filename'])[0]\n",
    "\n",
    "            # Calculate the event weights\n",
    "            totalWeights = get_xsec_weight(sample) * events.mcWeight * events.scaleFactor\n",
    "\n",
    "            if sample in [\"ttbar_lep\", \"Zee\", \"Zmumu\"]:\n",
    "                dataset = \"Background $Z,tt^{bar}$\"\n",
    "            elif sample == \"llll\":\n",
    "                dataset = \"Background $ZZ^{star}$\"\n",
    "            elif sample in [\"ggH125_ZZ4lep\", \"VBFH125_ZZ4lep\", \"WH125_ZZ4lep\", \"ZH125_ZZ4lep\"]:\n",
    "                dataset = \"Signal ($m_H$ = 125 GeV)\"\n",
    "\n",
    "            # Create and fill a weighted histogram for mllll\n",
    "            mllllhist = (\n",
    "                hist.Hist.new\n",
    "                .Reg(34, 80, 250, name='mllll', label='4-lepton invariant mass $\\mathrm{m_{4l}}$ [GeV]')\n",
    "                .StrCat(['Background $Z,tt^{bar}$', 'Background $ZZ^{star}$', 'Signal ($m_H$ = 125 GeV)'], name='dataset', label='Dataset')\n",
    "                .Weight()\n",
    "                .fill(mllll=mllll, dataset=dataset, weight=totalWeights)\n",
    "            )\n",
    "\n",
    "        return {\n",
    "            'entries': len(events),\n",
    "            'mllll': mllllhist\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31361726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def good_leptons(source: ObjectStream) -> ObjectStream:\n",
    "    '''Select out all good leptons from each event. Return their pt, eta, phi, and E, and other\n",
    "    things needed downstream.\n",
    "\n",
    "    Because uproot doesn't tie together the objects, we can't do any cuts at this point.\n",
    "    '''\n",
    "    return source.Select(lambda e:\n",
    "        {\n",
    "            'lep_pt': e.lep_pt,\n",
    "            'lep_eta': e.lep_eta,\n",
    "            'lep_phi': e.lep_phi,\n",
    "            'lep_energy': e.lep_E,\n",
    "            'lep_charge': e.lep_charge,\n",
    "            'lep_typeid': e.lep_type,\n",
    "            'mcWeight': e.mcWeight,\n",
    "            'scaleFactor': e.scaleFactor_ELE*e.scaleFactor_MUON*e.scaleFactor_LepTRIGGER*e.scaleFactor_PILEUP*1,\n",
    "        })\n",
    "\n",
    "def make_ds(name: str, query: ObjectStream):\n",
    "    '''Create a ServiceX Datasource for a particular ATLAS Open data file\n",
    "    '''\n",
    "    datasets = [ServiceXDataset(fileset[name], backend_name='uproot-fabric')]\n",
    "    return DataSource(query=query, metadata={'dataset': name}, datasets=datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec9ce14",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_analysis():\n",
    "    '''Run on a known analysis file/files and return the result.\n",
    "    Should be fine to start many of these at once.\n",
    "    '''\n",
    "    # Parse the dataset\n",
    "    ds_names = list(fileset.keys())\n",
    "\n",
    "    # Create the query\n",
    "    ds = ServiceXSourceUpROOT('cernopendata://dummy',  'mini', backend_name='uproot-fabric')\n",
    "    ds.return_qastle = True\n",
    "    leptons = good_leptons(ds)\n",
    "\n",
    "    # Get data source for this run.\n",
    "    # For running with Dask executor on coffea-casa, you can simply add:\n",
    "    # (please note https://github.com/CoffeaTeam/coffea/issues/611)\n",
    "    # if os.environ.get(\"LABEXTENTION_FACTORY_MODULE\") == \"coffea_casa\":\n",
    "    #   executor = DaskExecutor(client_addr=\"tls://localhost:8786\")\n",
    "    executor = LocalExecutor()\n",
    "\n",
    "    datasources = [make_ds(ds_name, leptons) for ds_name in ds_names]\n",
    "\n",
    "    # Create the analysis and we can run from there\n",
    "    analysis = HZZAnalysis()\n",
    "\n",
    "    async def run_updates_stream(accumulator_stream, name):\n",
    "        '''Run to get the last item in the stream'''\n",
    "        coffea_info = None\n",
    "        try:\n",
    "            async for coffea_info in accumulator_stream:\n",
    "                pass\n",
    "        except Exception as e:\n",
    "            raise Exception(f'Failure while processing {name}') from e\n",
    "        return coffea_info\n",
    "\n",
    "    all_plots = await asyncio.gather(*[run_updates_stream(executor.execute(analysis, source), source.metadata['dataset']) for source in datasources])\n",
    "\n",
    "    # Combine the MC plots\n",
    "    all_MC_plots = [p['mllll'] for p in all_plots[1:]]\n",
    "    MC_plot = all_MC_plots[0]\n",
    "    for p in all_MC_plots[1:]:\n",
    "        MC_plot += p\n",
    "\n",
    "    return {\n",
    "        'Data': all_plots[0]['mllll'],\n",
    "        'MC': MC_plot\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180cbad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "output = await run_analysis()\n",
    "finish_time = time.time()\n",
    "\n",
    "print(\"Total runtime in seconds: \" + str(finish_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ca8527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the histograms\n",
    "hist.Hist.plot1d(output['Data'], histtype='errorbar', color='black')\n",
    "hist.Hist.plot1d(output['MC'], stack=True, histtype='fill', color=['purple', 'red', 'cyan'])\n",
    "\n",
    "# Basic bin parameters\n",
    "xmin = 80\n",
    "xmax = 250\n",
    "step_size = 5\n",
    "\n",
    "bin_centers = np.arange(start=xmin+step_size/2, # The interval includes this value\n",
    "                        stop=xmax+step_size/2, # The interval doesn't include this value\n",
    "                        step=step_size ) # Spacing between values\n",
    "\n",
    "# or via bin_centers = output[\"Data\"].axes[0].centers\n",
    "\n",
    "# Calculate background statistical uncertainty\n",
    "# Remove the \"[:,:2]\" expressions to use all MC datasets with background and signal combined\n",
    "mc_tot_var = np.sum(output['MC'].variances()[:,:2], axis=1)\n",
    "mc_err = np.sqrt(mc_tot_var)\n",
    "mc_tot_height = np.sum(output['MC'].values()[:,:2], axis=1)\n",
    "\n",
    "# Plot background statistical uncertainty\n",
    "plt.bar(bin_centers, # x\n",
    "        2*mc_err, # heights\n",
    "        alpha=0.5, # half transparency\n",
    "        bottom=mc_tot_height-mc_err, color='none', \n",
    "        hatch=\"////\", width=step_size, label='Stat. Unc.')\n",
    "\n",
    "# Tune plot appearance\n",
    "main_axes = plt.gca()\n",
    "main_axes.set_xlim(left=xmin, right=xmax)\n",
    "main_axes.set_ylim(bottom=0, top=np.amax(output['Data'].values())*1.6)\n",
    "main_axes.set_ylabel('Events / '+str(step_size)+' GeV')\n",
    "main_axes.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b3d5b5",
   "metadata": {},
   "source": [
    "### Save histograms with uproot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31ea1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75837f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"histograms.root\"\n",
    "with uproot.recreate(file_name) as f:\n",
    "    f[\"data\"] = output[\"Data\"][:,\"Data\"]\n",
    "    f[\"Z_tt\"] = output[\"MC\"][:, \"Background $Z,tt^{bar}$\"]\n",
    "    f[\"ZZ\"] = output[\"MC\"][:, \"Background $ZZ^{star}$\"]\n",
    "    f[\"signal\"] = output[\"MC\"][:, \"Signal ($m_H$ = 125 GeV)\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
